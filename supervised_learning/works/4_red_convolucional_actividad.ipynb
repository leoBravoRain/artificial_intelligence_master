{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Actividad: Clasificación de imágenes naturales\n",
    "\n",
    "¿Podemos clasificar imágenes naturales usando redes neuronales convolucionales?\n",
    "\n",
    "En esta tarea usaremos la base de datos [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "Responda las preguntas y realice las actividades en cada uno de los bloques\n",
    "\n",
    "Entregas al correo phuijse@inf.uach.cl hasta el Viernes 27, 11:20 AM\n",
    "\n",
    "Se trabajará en grupos de dos personas: se entrega un notebook completo por grupo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base de datos\n",
    "\n",
    "Descargue la base de datos usando `torchvision`\n",
    "\n",
    "- ¿Cuántas categorías existen? \n",
    "\n",
    "Existen 10 categorías dentro del dataset\n",
    "\n",
    "- ¿Cuántos ejemplos hay por categoría?\n",
    "\n",
    "Se tienen 6000 datos por categoría\n",
    "\n",
    "- ¿Cúantos ejemplos hay en el conjunto de entrenamiento? ¿Cúantos hay en el de test?\n",
    "\n",
    "Para los datos de entrenamiento tnemos 50000 datos, y el conjunto de test tendremos 10000\n",
    "\n",
    "- ¿De que tamaño son las imágenes?\n",
    "\n",
    "El tamaño por cada imagen es de 32x32\n",
    "\n",
    "- Muestre 5 ejemplos seleccionados aleatoriamente de cada categoría\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: home/leo/datasets\n",
       "    Split: Train"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: home/leo/datasets\n",
       "    Split: Test"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='home/leo/datasets', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='home/leo/datasets', train=False,download=True, transform=torchvision.transforms.ToTensor())\n",
    "display(cifar10_train)\n",
    "display(cifar10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
       "         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
       "         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
       "         ...,\n",
       "         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
       "         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
       "         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n",
       "\n",
       "        [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n",
       "         [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n",
       "         [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n",
       "         ...,\n",
       "         [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n",
       "         [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n",
       "         [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n",
       "\n",
       "        [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n",
       "         [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n",
       "         [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n",
       "         ...,\n",
       "         [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n",
       "         [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n",
       "         [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1961, 0.2000, 0.1647,  ..., 0.1882, 0.0706, 0.0549],\n",
       "         [0.3373, 0.3608, 0.3216,  ..., 0.1804, 0.0667, 0.0431],\n",
       "         [0.1686, 0.1686, 0.2000,  ..., 0.1647, 0.0392, 0.0235],\n",
       "         ...,\n",
       "         [0.8627, 0.8196, 0.7804,  ..., 0.6941, 0.6902, 0.6863],\n",
       "         [0.7373, 0.7137, 0.7137,  ..., 0.6902, 0.6863, 0.6824],\n",
       "         [0.7373, 0.7216, 0.7294,  ..., 0.6902, 0.6902, 0.6784]],\n",
       "\n",
       "        [[0.2510, 0.2471, 0.2157,  ..., 0.1765, 0.0706, 0.0588],\n",
       "         [0.4196, 0.4314, 0.3882,  ..., 0.1686, 0.0667, 0.0471],\n",
       "         [0.2353, 0.2196, 0.2549,  ..., 0.1529, 0.0392, 0.0235],\n",
       "         ...,\n",
       "         [0.6471, 0.6824, 0.6745,  ..., 0.6706, 0.6667, 0.6588],\n",
       "         [0.6980, 0.6667, 0.6314,  ..., 0.6588, 0.6549, 0.6510],\n",
       "         [0.6549, 0.6392, 0.6549,  ..., 0.6627, 0.6588, 0.6471]],\n",
       "\n",
       "        [[0.1451, 0.1608, 0.1608,  ..., 0.1647, 0.0549, 0.0471],\n",
       "         [0.2627, 0.2980, 0.2627,  ..., 0.1608, 0.0549, 0.0392],\n",
       "         [0.1647, 0.1608, 0.1804,  ..., 0.1451, 0.0353, 0.0196],\n",
       "         ...,\n",
       "         [0.5922, 0.6471, 0.6431,  ..., 0.6588, 0.6549, 0.6510],\n",
       "         [0.6353, 0.6353, 0.6157,  ..., 0.6510, 0.6471, 0.6431],\n",
       "         [0.6353, 0.6157, 0.6275,  ..., 0.6510, 0.6510, 0.6353]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0627, 0.0667, 0.0667,  ..., 0.2431, 0.2157, 0.2000],\n",
       "         [0.0627, 0.0627, 0.0588,  ..., 0.2431, 0.2275, 0.2039],\n",
       "         [0.0627, 0.0588, 0.0588,  ..., 0.2235, 0.2314, 0.2196],\n",
       "         ...,\n",
       "         [0.3765, 0.4471, 0.4667,  ..., 0.5020, 0.4706, 0.4588],\n",
       "         [0.4627, 0.3922, 0.4471,  ..., 0.5451, 0.5137, 0.4745],\n",
       "         [0.5647, 0.5333, 0.4118,  ..., 0.5686, 0.5373, 0.5137]],\n",
       "\n",
       "        [[0.2980, 0.3020, 0.3020,  ..., 0.4157, 0.3882, 0.3686],\n",
       "         [0.2980, 0.2980, 0.2941,  ..., 0.4275, 0.4118, 0.3922],\n",
       "         [0.2980, 0.2941, 0.2941,  ..., 0.4314, 0.4353, 0.4275],\n",
       "         ...,\n",
       "         [0.4314, 0.4980, 0.5176,  ..., 0.5294, 0.5098, 0.5137],\n",
       "         [0.5176, 0.4431, 0.4941,  ..., 0.5725, 0.5490, 0.5255],\n",
       "         [0.5804, 0.5490, 0.4471,  ..., 0.5922, 0.5647, 0.5529]],\n",
       "\n",
       "        [[0.2902, 0.2941, 0.2941,  ..., 0.3412, 0.3137, 0.2941],\n",
       "         [0.2902, 0.2902, 0.2902,  ..., 0.3294, 0.3137, 0.2941],\n",
       "         [0.2902, 0.2863, 0.2863,  ..., 0.3098, 0.3137, 0.3059],\n",
       "         ...,\n",
       "         [0.5412, 0.6235, 0.6549,  ..., 0.6000, 0.5686, 0.5569],\n",
       "         [0.6235, 0.5686, 0.6392,  ..., 0.6235, 0.6000, 0.5765],\n",
       "         [0.6784, 0.6588, 0.5608,  ..., 0.6667, 0.6392, 0.6196]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2078, 0.2118, 0.2196,  ..., 0.1843, 0.1608, 0.0941],\n",
       "         [0.1804, 0.2078, 0.2118,  ..., 0.1647, 0.1529, 0.1098],\n",
       "         [0.1765, 0.1961, 0.1804,  ..., 0.1490, 0.1412, 0.1137],\n",
       "         ...,\n",
       "         [0.2784, 0.2902, 0.3137,  ..., 0.2000, 0.1804, 0.1922],\n",
       "         [0.2941, 0.3098, 0.3176,  ..., 0.2392, 0.2510, 0.1882],\n",
       "         [0.3333, 0.3333, 0.3373,  ..., 0.2392, 0.2510, 0.1922]],\n",
       "\n",
       "        [[0.2549, 0.2471, 0.2353,  ..., 0.2000, 0.1765, 0.1098],\n",
       "         [0.2314, 0.2431, 0.2314,  ..., 0.1804, 0.1686, 0.1255],\n",
       "         [0.2314, 0.2353, 0.2039,  ..., 0.1647, 0.1569, 0.1294],\n",
       "         ...,\n",
       "         [0.3255, 0.3255, 0.3333,  ..., 0.2118, 0.1922, 0.1961],\n",
       "         [0.3216, 0.3333, 0.3333,  ..., 0.2549, 0.2627, 0.1961],\n",
       "         [0.3255, 0.3294, 0.3373,  ..., 0.2549, 0.2627, 0.1961]],\n",
       "\n",
       "        [[0.2078, 0.2039, 0.1961,  ..., 0.1961, 0.1725, 0.1059],\n",
       "         [0.1608, 0.1765, 0.1725,  ..., 0.1765, 0.1647, 0.1216],\n",
       "         [0.1490, 0.1608, 0.1333,  ..., 0.1608, 0.1529, 0.1255],\n",
       "         ...,\n",
       "         [0.2588, 0.2588, 0.2627,  ..., 0.1294, 0.1333, 0.1608],\n",
       "         [0.2627, 0.2706, 0.2627,  ..., 0.1608, 0.1882, 0.1608],\n",
       "         [0.2784, 0.2784, 0.2745,  ..., 0.1529, 0.1804, 0.1608]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.5255, 0.4431, 0.3373,  ..., 0.9647, 0.9882, 1.0000],\n",
       "         [0.4118, 0.3137, 0.2706,  ..., 0.9843, 0.9882, 1.0000],\n",
       "         [0.3333, 0.2510, 0.2275,  ..., 0.9647, 0.9490, 0.9647]],\n",
       "\n",
       "        [[1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.2784, 0.2196, 0.1882,  ..., 0.9412, 0.9686, 0.9961],\n",
       "         [0.2000, 0.1294, 0.1490,  ..., 1.0000, 0.9961, 1.0000],\n",
       "         [0.1608, 0.1137, 0.1137,  ..., 0.9412, 0.8980, 0.8784]],\n",
       "\n",
       "        [[1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.1490, 0.1255, 0.0863,  ..., 0.9137, 0.9529, 0.9961],\n",
       "         [0.0863, 0.0549, 0.0549,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0588, 0.0510, 0.0314,  ..., 0.8784, 0.8275, 0.8353]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(cifar10_train[0][0])\n",
    "display(cifar10_train[32][0])\n",
    "display(cifar10_train[50][0])\n",
    "display(cifar10_train[10][0])\n",
    "display(cifar10_train[131][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detalles del dataset\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "\n",
    "Construya `dataloaders` para el conjunto de entrenamiento y validación a partir del dataset `cifar10_train`\n",
    "\n",
    "Construya un `dataloader` para el conjunto de test a partir del dataset `cifar10_test`\n",
    "\n",
    "Use `shuffling` y tamaño de batch 32 para el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set de entrenamiento y validación estratíficados\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(train_size=0.75).split(cifar10_train.data, cifar10_train.targets)\n",
    "train_idx, valid_idx = next(sss)\n",
    "\n",
    "# Create train data loader\n",
    "train_dataset = Subset(cifar10_train, train_idx)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "# Create valid data loader\n",
    "valid_dataset = Subset(cifar10_train, valid_idx)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=256)\n",
    "\n",
    "# Create test data loader\n",
    "test_loader = DataLoader(cifar10_test, shuffle=False, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal convolucional\n",
    "\n",
    "Construya una clase que herede de `torch.nn.Module` y que implemente una red neuronal convolucional\n",
    "\n",
    "Tome como base la arquitectura [Lenet5](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) \n",
    "- Tres capas convolucionales cada una seguida de una capa de max-pooling\n",
    "- Dos capas completamente conectadas\n",
    "- Función de activación ReLU en todas las capas excepto en la última \n",
    "- Función de costo `CrossEntropyLoss`\n",
    "- Optimizador `Adam`\n",
    "\n",
    "Entrene su red en la base de datos CIFAR10\n",
    "- Ajuste los parámetros con el conjunto de entrenamiento\n",
    "- Evite el sobreajuste y calibre los hyper-parámetros en el conjunto de validación\n",
    "- Muestre la calidad de su modelo evaluando en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "# neural arquitechture\n",
    "class diego_net_leo(torch.nn.Module):\n",
    "\n",
    "    # init\n",
    "    def __init__(self, n_hidden=20, n_filters=8):\n",
    "        super(diego_net_leo, self).__init__()\n",
    "        # Extracción de características\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=n_filters, kernel_size=3)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=n_filters, out_channels=16, kernel_size=3)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3)\n",
    "        self.mpool = torch.nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "                \n",
    "        # Clasificación\n",
    "        self.fc1 = torch.nn.Linear(in_features=16, out_features=n_hidden)\n",
    "        self.fc2 = torch.nn.Linear(in_features=n_hidden, out_features=10)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"input: \",x.shape)\n",
    "        #return x\n",
    "        z = self.activation(self.conv1(x))\n",
    "        \n",
    "        #print(\"conv 1: \", z.shape)\n",
    "        \n",
    "        z = self.mpool(z)\n",
    "        \n",
    "        #print(\"pool 1: \", z.shape)\n",
    "        \n",
    "        z = self.activation(self.conv2(z))\n",
    "        \n",
    "        #print(\"conv 2: \", z.shape)\n",
    "        \n",
    "        z = self.mpool(z)\n",
    "        \n",
    "        #print(\"pool 2: \", z.shape)\n",
    "        \n",
    "        z = self.activation(self.conv3(z))\n",
    "        \n",
    "        #print(\"conv 3: \", z.shape)\n",
    "        \n",
    "        z = self.mpool(z)\n",
    "        \n",
    "        #print(\"pool 3: \", z.shape)\n",
    "        \n",
    "        z = z.reshape(-1, 16*1*1)\n",
    "        \n",
    "        #print(\"reshape: \", z.shape)\n",
    "        \n",
    "        z = self.activation(self.fc1(z))\n",
    "        \n",
    "        #print(\"fc1: \", z.shape)\n",
    "        \n",
    "        z = self.fc2(z)\n",
    "        \n",
    "        #print(\"fc2: \", z.shape)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "# model = diego_net_leo()\n",
    "# display(model.forward(cifar10_train[0][0].unsqueeze(0)))\n",
    "# display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "# model = diego_net_leo()\n",
    "model = diego_net_leo().cuda()\n",
    "# display(model.forward(cifar10_train[0][0].unsqueeze(0)))\n",
    "# display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf58e23a7fe84d97ba7397011b27bd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "new best model\n",
      "epoch:  1\n",
      "epoch:  2\n",
      "epoch:  3\n",
      "epoch:  4\n",
      "epoch:  5\n",
      "new best model\n",
      "epoch:  6\n",
      "epoch:  7\n",
      "epoch:  8\n",
      "epoch:  9\n",
      "epoch:  10\n",
      "new best model\n",
      "epoch:  11\n",
      "epoch:  12\n",
      "epoch:  13\n",
      "epoch:  14\n",
      "epoch:  15\n",
      "new best model\n",
      "epoch:  16\n",
      "epoch:  17\n",
      "epoch:  18\n",
      "epoch:  19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "use_gpu = True\n",
    "# nepochs = 2\n",
    "best_valid = np.inf\n",
    "writer = SummaryWriter(log_dir=\"/tmp/tensorboard/red_convolucional_simple\"+str(time.time()),   flush_secs=20)\n",
    "\n",
    "# Enviar modelo a la GPU\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "for k in tqdm_notebook(range(nepochs)):\n",
    "    \n",
    "    print(\"epoch: \", k)\n",
    "    \n",
    "    # Entrenamiento\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    \n",
    "    for mbdata, mblabel in train_loader:\n",
    "        \n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "        # Calcular gradientes\n",
    "        optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "        # Actualizar parámetros\n",
    "        optimizer.step()\n",
    "    # Enviar información a tensorboard\n",
    "    writer.add_scalar('Train/Loss', epoch_loss/len(train_idx), k)\n",
    "    writer.add_scalar('Train/Acc', epoch_acc/len(train_idx), k)\n",
    "    # Validación\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for mbdata, mblabel in valid_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "    # Enviar información a tensorboard\n",
    "    writer.add_scalar('Valid/Loss', epoch_loss/len(valid_idx), k)\n",
    "    writer.add_scalar('Valid/Acc', epoch_acc/len(valid_idx), k)\n",
    "    \n",
    "    if k % 5 == 0:\n",
    "        if epoch_loss/len(valid_idx) < best_valid:\n",
    "            print('new best model')\n",
    "            best_valid = epoch_loss/len(valid_idx)\n",
    "            torch.save({'epoca': k,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),                        \n",
    "                        'loss': epoch_loss/len(valid_idx)}, \n",
    "                       'home/leo/models/best_model.pt') # MODIFICA LA RUTA \n",
    "\n",
    "# Retornar modelo a la CPU\n",
    "if use_gpu:\n",
    "    model = model.cpu()\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recovering best model\n",
    "model = diego_net_leo()\n",
    "\n",
    "model.load_state_dict(torch.load('home/leo/models/best_model.pt')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[591,  37,  79,  23,   9,  11,  31,  16, 149,  54],\n",
       "       [ 64, 574,   4,  14,  11,   3,  23,  15,  59, 233],\n",
       "       [ 89,   3, 323, 135,  88, 102, 165,  45,  29,  21],\n",
       "       [ 25,  12,  99, 332,  23, 236, 162,  42,  33,  36],\n",
       "       [ 45,  17, 158,  89, 248,  74, 220, 122,  13,  14],\n",
       "       [ 15,   2,  87, 227,  29, 473,  74,  59,  17,  17],\n",
       "       [ 11,   8,  34,  74,  32,  25, 778,  15,   8,  15],\n",
       "       [ 23,  16,  36,  84,  50, 151,  38, 541,  12,  49],\n",
       "       [158,  51,  23,  35,   2,   6,  11,   5, 639,  70],\n",
       "       [ 32, 116,   4,  19,   3,   7,  28,  12,  72, 707]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.59      0.58      1000\n",
      "           1       0.69      0.57      0.63      1000\n",
      "           2       0.38      0.32      0.35      1000\n",
      "           3       0.32      0.33      0.33      1000\n",
      "           4       0.50      0.25      0.33      1000\n",
      "           5       0.43      0.47      0.45      1000\n",
      "           6       0.51      0.78      0.62      1000\n",
      "           7       0.62      0.54      0.58      1000\n",
      "           8       0.62      0.64      0.63      1000\n",
      "           9       0.58      0.71      0.64      1000\n",
      "\n",
      "    accuracy                           0.52     10000\n",
      "   macro avg       0.52      0.52      0.51     10000\n",
      "weighted avg       0.52      0.52      0.51     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing in test loader\n",
    "# test_loader = DataLoader(cifar10_test, shuffle=False, batch_size=512)\n",
    "# test_targets = cifar10_test.targets.numpy()\n",
    "test_targets = cifar10_test.targets\n",
    "prediction_test = []\n",
    "for mbdata, label in test_loader:\n",
    "    logits = model.forward(mbdata)\n",
    "    prediction_test.append(torch.argmax(logits, dim=1).detach().numpy())\n",
    "\n",
    "prediction_test = np.concatenate(prediction_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(test_targets, prediction_test)\n",
    "display(cm)\n",
    "\n",
    "print(classification_report(test_targets, prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de resultados sobre testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aumentación de datos\n",
    "\n",
    "Use `torchvision.transforms` para implementar una serie de transformaciones que aumenten el dataset de entrenamiento\n",
    "\n",
    "Compare los resultados de la red entrenada en datos aumentados con la red entrenada en el conjunto regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.RandomHorizontalFlip(),\n",
    "                                            torchvision.transforms.RandomRotation(degrees=50),\n",
    "                                            torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, \n",
    "                                                                               saturation=0.5, hue=0.0),\n",
    "                                            torchvision.transforms.ToTensor()\n",
    "                                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new train dataset\n",
    "cifar10_train_augmented = torchvision.datasets.CIFAR10(\n",
    "    root='home/leo/datasets', \n",
    "    train=True, download=False, \n",
    "    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new loader\n",
    "\n",
    "# Set de entrenamiento y validación estratíficados\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(train_size=0.75).split(cifar10_train_augmented.data, cifar10_train_augmented.targets)\n",
    "train_idx, valid_idx = next(sss)\n",
    "\n",
    "# Create train data loader\n",
    "train_dataset_augmented = Subset(cifar10_train_augmented, train_idx)\n",
    "train_loader_augmented = DataLoader(train_dataset_augmented, shuffle=True, batch_size=32)\n",
    "\n",
    "# Create valid data loader\n",
    "valid_dataset_augmented = Subset(cifar10_train_augmented, valid_idx)\n",
    "valid_loader_augmented = DataLoader(valid_dataset_augmented, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model \n",
    "model_augmented = diego_net_leo().cuda()\n",
    "\n",
    "optimizer_augmented = torch.optim.Adam(model_augmented.parameters(), lr=1e-3, amsgrad=True)\n",
    "\n",
    "# define loss function\n",
    "criterion_augmented = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732b7d3289384248a5ab6a1a75b43993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1b8630c04d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmbdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmblabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader_augmented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    917\u001b[0m         transform = self.get_params(self.brightness, self.contrast,\n\u001b[1;32m    918\u001b[0m                                     self.saturation, self.hue)\n\u001b[0;32m--> 919\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbrightness\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mbrightness_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrightness\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrightness\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_brightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrightness_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontrast\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36madjust_brightness\u001b[0;34m(img, brightness_factor)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m     \u001b[0menhancer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEnhance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBrightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menhance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrightness_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/PIL/ImageEnhance.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegenerate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'A'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2391\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2392\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2393\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model with augmented data\n",
    "use_gpu = True\n",
    "# nepochs = 2\n",
    "best_valid = np.inf\n",
    "# writer = SummaryWriter(log_dir=\"/tmp/tensorboard/red_convolucional_simple/\"+str(time()),\n",
    "#                        flush_secs=20)\n",
    "\n",
    "# Enviar modelo a la GPU\n",
    "if use_gpu:\n",
    "    model_augmented = model_augmented.cuda()\n",
    "\n",
    "for k in tqdm_notebook(range(nepochs)):\n",
    "    \n",
    "    # Entrenamiento\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    \n",
    "    for mbdata, mblabel in train_loader_augmented:\n",
    "        \n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model_augmented.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion_augmented(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "        # Calcular gradientes\n",
    "        optimizer_augmented.zero_grad()        \n",
    "        loss.backward()\n",
    "        # Actualizar parámetros\n",
    "        optimizer_augmented.step()\n",
    "    # Enviar información a tensorboard\n",
    "#     writer.add_scalar('Train/Loss', epoch_loss/len(train_idx), k)\n",
    "#     writer.add_scalar('Train/Acc', epoch_acc/len(train_idx), k)\n",
    "    # Validación\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for mbdata, mblabel in valid_loader_augmented:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model_augmented.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion_augmented(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "#     # Enviar información a tensorboard\n",
    "#     writer.add_scalar('Valid/Loss', epoch_loss/len(valid_idx), k)\n",
    "#     writer.add_scalar('Valid/Acc', epoch_acc/len(valid_idx), k)\n",
    "    \n",
    "    if k % 5 == 0:\n",
    "        if epoch_loss/len(valid_idx) < best_valid:\n",
    "            best_valid = epoch_loss/len(valid_idx)\n",
    "            torch.save({'epoca': k,\n",
    "                        'model_state_dict': model_augmented.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer_augmented.state_dict(),                        \n",
    "                        'loss': epoch_loss/len(valid_idx)}, \n",
    "                       'home/leo/models/best_model_augmented.pt') # MODIFICA LA RUTA \n",
    "\n",
    "# Retornar modelo a la CPU\n",
    "if use_gpu:\n",
    "    model_augmented = model_augmented.cpu()\n",
    "    \n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recovering best model\n",
    "model_augmented = diego_net_leo()\n",
    "\n",
    "model_augmented.load_state_dict(torch.load('home/leo/models/best_model_augmented.pt')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing in test loader\n",
    "# test_targets = test_loader.targets.numpy()\n",
    "test_targets = cifar10_test.targets\n",
    "prediction_test_augmented = []\n",
    "for mbdata, label in test_loader:\n",
    "    logits_ = model_augmented.forward(mbdata)\n",
    "    prediction_test_augmented.append(torch.argmax(logits_, dim=1).detach().numpy())\n",
    "\n",
    "# print(prediction_test_augmented)\n",
    "\n",
    "    \n",
    "prediction_test_augmented = np.concatenate(prediction_test_augmented)\n",
    "\n",
    "# print(prediction_test_augmented)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(test_targets, prediction_test_augmented)\n",
    "display(cm)\n",
    "\n",
    "print(classification_report(test_targets, prediction_test_augmented))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparación entrenamiento con y sin aumentación\n",
    "\n",
    "Agregar analisis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuestos (bonus)\n",
    "\n",
    "- Implemente una arquitectura de tipo [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ)\n",
    "- Pruebe la eficacia de Dropout o Regularización L2 en los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
