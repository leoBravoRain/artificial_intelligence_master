{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Actividad: Clasificación de imágenes naturales\n",
    "\n",
    "¿Podemos clasificar imágenes naturales usando redes neuronales convolucionales?\n",
    "\n",
    "En esta tarea usaremos la base de datos [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "Responda las preguntas y realice las actividades en cada uno de los bloques\n",
    "\n",
    "Entregas al correo phuijse@inf.uach.cl hasta el Viernes 27, 11:20 AM\n",
    "\n",
    "Se trabajará en grupos de dos personas: se entrega un notebook completo por grupo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base de datos\n",
    "\n",
    "Descargue la base de datos usando `torchvision`\n",
    "\n",
    "- ¿Cuántas categorías existen? \n",
    "\n",
    "Existen 10 categorías dentro del dataset\n",
    "\n",
    "- ¿Cuántos ejemplos hay por categoría?\n",
    "\n",
    "Se tienen 6000 datos por categoría\n",
    "\n",
    "- ¿Cúantos ejemplos hay en el conjunto de entrenamiento? ¿Cúantos hay en el de test?\n",
    "\n",
    "Para los datos de entrenamiento tnemos 50000 datos, y el conjunto de test tendremos 10000\n",
    "\n",
    "- ¿De que tamaño son las imágenes?\n",
    "\n",
    "El tamaño por cada imagen es de 32x32\n",
    "\n",
    "- Muestre 5 ejemplos seleccionados aleatoriamente de cada categoría\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: home/leo/datasets\n",
       "    Split: Train"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: home/leo/datasets\n",
       "    Split: Test"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='home/leo/datasets', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='home/leo/datasets', train=False,download=True, transform=torchvision.transforms.ToTensor())\n",
    "display(cifar10_train)\n",
    "display(cifar10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
       "         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
       "         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
       "         ...,\n",
       "         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
       "         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
       "         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n",
       "\n",
       "        [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n",
       "         [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n",
       "         [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n",
       "         ...,\n",
       "         [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n",
       "         [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n",
       "         [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n",
       "\n",
       "        [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n",
       "         [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n",
       "         [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n",
       "         ...,\n",
       "         [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n",
       "         [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n",
       "         [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1961, 0.2000, 0.1647,  ..., 0.1882, 0.0706, 0.0549],\n",
       "         [0.3373, 0.3608, 0.3216,  ..., 0.1804, 0.0667, 0.0431],\n",
       "         [0.1686, 0.1686, 0.2000,  ..., 0.1647, 0.0392, 0.0235],\n",
       "         ...,\n",
       "         [0.8627, 0.8196, 0.7804,  ..., 0.6941, 0.6902, 0.6863],\n",
       "         [0.7373, 0.7137, 0.7137,  ..., 0.6902, 0.6863, 0.6824],\n",
       "         [0.7373, 0.7216, 0.7294,  ..., 0.6902, 0.6902, 0.6784]],\n",
       "\n",
       "        [[0.2510, 0.2471, 0.2157,  ..., 0.1765, 0.0706, 0.0588],\n",
       "         [0.4196, 0.4314, 0.3882,  ..., 0.1686, 0.0667, 0.0471],\n",
       "         [0.2353, 0.2196, 0.2549,  ..., 0.1529, 0.0392, 0.0235],\n",
       "         ...,\n",
       "         [0.6471, 0.6824, 0.6745,  ..., 0.6706, 0.6667, 0.6588],\n",
       "         [0.6980, 0.6667, 0.6314,  ..., 0.6588, 0.6549, 0.6510],\n",
       "         [0.6549, 0.6392, 0.6549,  ..., 0.6627, 0.6588, 0.6471]],\n",
       "\n",
       "        [[0.1451, 0.1608, 0.1608,  ..., 0.1647, 0.0549, 0.0471],\n",
       "         [0.2627, 0.2980, 0.2627,  ..., 0.1608, 0.0549, 0.0392],\n",
       "         [0.1647, 0.1608, 0.1804,  ..., 0.1451, 0.0353, 0.0196],\n",
       "         ...,\n",
       "         [0.5922, 0.6471, 0.6431,  ..., 0.6588, 0.6549, 0.6510],\n",
       "         [0.6353, 0.6353, 0.6157,  ..., 0.6510, 0.6471, 0.6431],\n",
       "         [0.6353, 0.6157, 0.6275,  ..., 0.6510, 0.6510, 0.6353]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0627, 0.0667, 0.0667,  ..., 0.2431, 0.2157, 0.2000],\n",
       "         [0.0627, 0.0627, 0.0588,  ..., 0.2431, 0.2275, 0.2039],\n",
       "         [0.0627, 0.0588, 0.0588,  ..., 0.2235, 0.2314, 0.2196],\n",
       "         ...,\n",
       "         [0.3765, 0.4471, 0.4667,  ..., 0.5020, 0.4706, 0.4588],\n",
       "         [0.4627, 0.3922, 0.4471,  ..., 0.5451, 0.5137, 0.4745],\n",
       "         [0.5647, 0.5333, 0.4118,  ..., 0.5686, 0.5373, 0.5137]],\n",
       "\n",
       "        [[0.2980, 0.3020, 0.3020,  ..., 0.4157, 0.3882, 0.3686],\n",
       "         [0.2980, 0.2980, 0.2941,  ..., 0.4275, 0.4118, 0.3922],\n",
       "         [0.2980, 0.2941, 0.2941,  ..., 0.4314, 0.4353, 0.4275],\n",
       "         ...,\n",
       "         [0.4314, 0.4980, 0.5176,  ..., 0.5294, 0.5098, 0.5137],\n",
       "         [0.5176, 0.4431, 0.4941,  ..., 0.5725, 0.5490, 0.5255],\n",
       "         [0.5804, 0.5490, 0.4471,  ..., 0.5922, 0.5647, 0.5529]],\n",
       "\n",
       "        [[0.2902, 0.2941, 0.2941,  ..., 0.3412, 0.3137, 0.2941],\n",
       "         [0.2902, 0.2902, 0.2902,  ..., 0.3294, 0.3137, 0.2941],\n",
       "         [0.2902, 0.2863, 0.2863,  ..., 0.3098, 0.3137, 0.3059],\n",
       "         ...,\n",
       "         [0.5412, 0.6235, 0.6549,  ..., 0.6000, 0.5686, 0.5569],\n",
       "         [0.6235, 0.5686, 0.6392,  ..., 0.6235, 0.6000, 0.5765],\n",
       "         [0.6784, 0.6588, 0.5608,  ..., 0.6667, 0.6392, 0.6196]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2078, 0.2118, 0.2196,  ..., 0.1843, 0.1608, 0.0941],\n",
       "         [0.1804, 0.2078, 0.2118,  ..., 0.1647, 0.1529, 0.1098],\n",
       "         [0.1765, 0.1961, 0.1804,  ..., 0.1490, 0.1412, 0.1137],\n",
       "         ...,\n",
       "         [0.2784, 0.2902, 0.3137,  ..., 0.2000, 0.1804, 0.1922],\n",
       "         [0.2941, 0.3098, 0.3176,  ..., 0.2392, 0.2510, 0.1882],\n",
       "         [0.3333, 0.3333, 0.3373,  ..., 0.2392, 0.2510, 0.1922]],\n",
       "\n",
       "        [[0.2549, 0.2471, 0.2353,  ..., 0.2000, 0.1765, 0.1098],\n",
       "         [0.2314, 0.2431, 0.2314,  ..., 0.1804, 0.1686, 0.1255],\n",
       "         [0.2314, 0.2353, 0.2039,  ..., 0.1647, 0.1569, 0.1294],\n",
       "         ...,\n",
       "         [0.3255, 0.3255, 0.3333,  ..., 0.2118, 0.1922, 0.1961],\n",
       "         [0.3216, 0.3333, 0.3333,  ..., 0.2549, 0.2627, 0.1961],\n",
       "         [0.3255, 0.3294, 0.3373,  ..., 0.2549, 0.2627, 0.1961]],\n",
       "\n",
       "        [[0.2078, 0.2039, 0.1961,  ..., 0.1961, 0.1725, 0.1059],\n",
       "         [0.1608, 0.1765, 0.1725,  ..., 0.1765, 0.1647, 0.1216],\n",
       "         [0.1490, 0.1608, 0.1333,  ..., 0.1608, 0.1529, 0.1255],\n",
       "         ...,\n",
       "         [0.2588, 0.2588, 0.2627,  ..., 0.1294, 0.1333, 0.1608],\n",
       "         [0.2627, 0.2706, 0.2627,  ..., 0.1608, 0.1882, 0.1608],\n",
       "         [0.2784, 0.2784, 0.2745,  ..., 0.1529, 0.1804, 0.1608]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.5255, 0.4431, 0.3373,  ..., 0.9647, 0.9882, 1.0000],\n",
       "         [0.4118, 0.3137, 0.2706,  ..., 0.9843, 0.9882, 1.0000],\n",
       "         [0.3333, 0.2510, 0.2275,  ..., 0.9647, 0.9490, 0.9647]],\n",
       "\n",
       "        [[1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.2784, 0.2196, 0.1882,  ..., 0.9412, 0.9686, 0.9961],\n",
       "         [0.2000, 0.1294, 0.1490,  ..., 1.0000, 0.9961, 1.0000],\n",
       "         [0.1608, 0.1137, 0.1137,  ..., 0.9412, 0.8980, 0.8784]],\n",
       "\n",
       "        [[1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.1490, 0.1255, 0.0863,  ..., 0.9137, 0.9529, 0.9961],\n",
       "         [0.0863, 0.0549, 0.0549,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0588, 0.0510, 0.0314,  ..., 0.8784, 0.8275, 0.8353]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(cifar10_train[0][0])\n",
    "display(cifar10_train[32][0])\n",
    "display(cifar10_train[50][0])\n",
    "display(cifar10_train[10][0])\n",
    "display(cifar10_train[131][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detalles del dataset\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "\n",
    "Construya `dataloaders` para el conjunto de entrenamiento y validación a partir del dataset `cifar10_train`\n",
    "\n",
    "Construya un `dataloader` para el conjunto de test a partir del dataset `cifar10_test`\n",
    "\n",
    "Use `shuffling` y tamaño de batch 32 para el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set de entrenamiento y validación estratíficados\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(train_size=0.75).split(cifar10_train.data, cifar10_train.targets)\n",
    "train_idx, valid_idx = next(sss)\n",
    "\n",
    "# Create train data loader\n",
    "train_dataset = Subset(cifar10_train, train_idx)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "# Create valid data loader\n",
    "valid_dataset = Subset(cifar10_train, valid_idx)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=256)\n",
    "\n",
    "# Create test data loader\n",
    "test_loader = DataLoader(cifar10_test, shuffle=False, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal convolucional\n",
    "\n",
    "Construya una clase que herede de `torch.nn.Module` y que implemente una red neuronal convolucional\n",
    "\n",
    "Tome como base la arquitectura [Lenet5](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) \n",
    "- Tres capas convolucionales cada una seguida de una capa de max-pooling\n",
    "- Dos capas completamente conectadas\n",
    "- Función de activación ReLU en todas las capas excepto en la última \n",
    "- Función de costo `CrossEntropyLoss`\n",
    "- Optimizador `Adam`\n",
    "\n",
    "Entrene su red en la base de datos CIFAR10\n",
    "- Ajuste los parámetros con el conjunto de entrenamiento\n",
    "- Evite el sobreajuste y calibre los hyper-parámetros en el conjunto de validación\n",
    "- Muestre la calidad de su modelo evaluando en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "# neural arquitechture\n",
    "class diego_net_leo(torch.nn.Module):\n",
    "\n",
    "    # init\n",
    "    def __init__(self, n_hidden=20, n_filters=8):\n",
    "        super(diego_net_leo, self).__init__()\n",
    "        # Extracción de características\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=n_filters, kernel_size=3)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=n_filters, out_channels=16, kernel_size=3)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3)\n",
    "        self.mpool = torch.nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "                \n",
    "        # Clasificación\n",
    "        self.fc1 = torch.nn.Linear(in_features=16, out_features=n_hidden)\n",
    "        self.fc2 = torch.nn.Linear(in_features=n_hidden, out_features=10)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"input: \",x.shape)\n",
    "        #return x\n",
    "        z = self.activation(self.conv1(x))\n",
    "        \n",
    "        #print(\"conv 1: \", z.shape)\n",
    "        \n",
    "        z = self.mpool(z)\n",
    "        \n",
    "        #print(\"pool 1: \", z.shape)\n",
    "        \n",
    "        z = self.activation(self.conv2(z))\n",
    "        \n",
    "        #print(\"conv 2: \", z.shape)\n",
    "        \n",
    "        z = self.mpool(z)\n",
    "        \n",
    "        #print(\"pool 2: \", z.shape)\n",
    "        \n",
    "        z = self.activation(self.conv3(z))\n",
    "        \n",
    "        #print(\"conv 3: \", z.shape)\n",
    "        \n",
    "        z = self.mpool(z)\n",
    "        \n",
    "        #print(\"pool 3: \", z.shape)\n",
    "        \n",
    "        z = z.reshape(-1, 16*1*1)\n",
    "        \n",
    "        #print(\"reshape: \", z.shape)\n",
    "        \n",
    "        z = self.activation(self.fc1(z))\n",
    "        \n",
    "        #print(\"fc1: \", z.shape)\n",
    "        \n",
    "        z = self.fc2(z)\n",
    "        \n",
    "        #print(\"fc2: \", z.shape)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "# model = diego_net_leo()\n",
    "# display(model.forward(cifar10_train[0][0].unsqueeze(0)))\n",
    "# display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "# model = diego_net_leo()\n",
    "model = diego_net_leo().cuda()\n",
    "# display(model.forward(cifar10_train[0][0].unsqueeze(0)))\n",
    "# display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1325bfcf864c6ea849bbe6d6a8309b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "use_gpu = True\n",
    "nepochs = 2\n",
    "best_valid = np.inf\n",
    "# writer = SummaryWriter(log_dir=\"/tmp/tensorboard/red_convolucional_simple/\"+str(time()),\n",
    "#                        flush_secs=20)\n",
    "\n",
    "# Enviar modelo a la GPU\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "for k in tqdm_notebook(range(nepochs)):\n",
    "    \n",
    "    # Entrenamiento\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    \n",
    "    for mbdata, mblabel in train_loader:\n",
    "        \n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "        # Calcular gradientes\n",
    "        optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "        # Actualizar parámetros\n",
    "        optimizer.step()\n",
    "    # Enviar información a tensorboard\n",
    "#     writer.add_scalar('Train/Loss', epoch_loss/len(train_idx), k)\n",
    "#     writer.add_scalar('Train/Acc', epoch_acc/len(train_idx), k)\n",
    "    # Validación\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for mbdata, mblabel in valid_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "    # Enviar información a tensorboard\n",
    "#     writer.add_scalar('Valid/Loss', epoch_loss/len(valid_idx), k)\n",
    "#     writer.add_scalar('Valid/Acc', epoch_acc/len(valid_idx), k)\n",
    "    \n",
    "    if k % 5 == 0:\n",
    "        if epoch_loss/len(valid_idx) < best_valid:\n",
    "            print('new best model')\n",
    "            best_valid = epoch_loss/len(valid_idx)\n",
    "            torch.save({'epoca': k,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),                        \n",
    "                        'loss': epoch_loss/len(valid_idx)}, \n",
    "                       'home/leo/models/best_model.pt') # MODIFICA LA RUTA \n",
    "\n",
    "# Retornar modelo a la CPU\n",
    "if use_gpu:\n",
    "    model = model.cpu()\n",
    "    \n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[310,  37,  91,  30,   4,  71,  24,  45, 322,  66],\n",
       "       [ 44, 255,  39,  19,   2,   6,  23,  81, 188, 343],\n",
       "       [ 42,   3, 229,  38, 162, 205, 180,  92,  30,  19],\n",
       "       [ 14,   4, 157,  41, 102, 407, 139, 102,  10,  24],\n",
       "       [ 13,   4, 151,  33, 240, 158, 268,  97,  21,  15],\n",
       "       [ 12,   0,  90,  38,  74, 564, 114,  87,   8,  13],\n",
       "       [  4,   0, 103,  18, 171,  76, 543,  60,   7,  18],\n",
       "       [  6,   2,  73,  32,  48, 265, 107, 400,  16,  51],\n",
       "       [133,  73,  85,  26,   2,  29,  14,  29, 523,  86],\n",
       "       [ 28,  95,  32,  32,   0,  27,  50, 131, 164, 441]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.31      0.39      1000\n",
      "           1       0.54      0.26      0.35      1000\n",
      "           2       0.22      0.23      0.22      1000\n",
      "           3       0.13      0.04      0.06      1000\n",
      "           4       0.30      0.24      0.27      1000\n",
      "           5       0.31      0.56      0.40      1000\n",
      "           6       0.37      0.54      0.44      1000\n",
      "           7       0.36      0.40      0.38      1000\n",
      "           8       0.41      0.52      0.46      1000\n",
      "           9       0.41      0.44      0.42      1000\n",
      "\n",
      "    accuracy                           0.35     10000\n",
      "   macro avg       0.36      0.35      0.34     10000\n",
      "weighted avg       0.36      0.35      0.34     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing in test loader\n",
    "# test_loader = DataLoader(cifar10_test, shuffle=False, batch_size=512)\n",
    "# test_targets = cifar10_test.targets.numpy()\n",
    "test_targets = cifar10_test.targets\n",
    "prediction_test = []\n",
    "for mbdata, label in test_loader:\n",
    "    logits = model.forward(mbdata)\n",
    "    prediction_test.append(torch.argmax(logits, dim=1).detach().numpy())\n",
    "\n",
    "prediction_test = np.concatenate(prediction_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(test_targets, prediction_test)\n",
    "display(cm)\n",
    "\n",
    "print(classification_report(test_targets, prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de resultados sobre testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aumentación de datos\n",
    "\n",
    "Use `torchvision.transforms` para implementar una serie de transformaciones que aumenten el dataset de entrenamiento\n",
    "\n",
    "Compare los resultados de la red entrenada en datos aumentados con la red entrenada en el conjunto regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.RandomHorizontalFlip(),\n",
    "                                            torchvision.transforms.RandomRotation(degrees=50),\n",
    "                                            torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, \n",
    "                                                                               saturation=0.5, hue=0.0),\n",
    "                                            torchvision.transforms.ToTensor()\n",
    "                                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new train dataset\n",
    "cifar10_train_augmented = torchvision.datasets.CIFAR10(\n",
    "    root='home/leo/datasets', \n",
    "    train=True, download=False, \n",
    "    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new loader\n",
    "\n",
    "# Set de entrenamiento y validación estratíficados\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(train_size=0.75).split(cifar10_train_augmented.data, cifar10_train_augmented.targets)\n",
    "train_idx, valid_idx = next(sss)\n",
    "\n",
    "# Create train data loader\n",
    "train_dataset_augmented = Subset(cifar10_train_augmented, train_idx)\n",
    "train_loader_augmented = DataLoader(train_dataset_augmented, shuffle=True, batch_size=32)\n",
    "\n",
    "# Create valid data loader\n",
    "valid_dataset_augmented = Subset(cifar10_train_augmented, valid_idx)\n",
    "valid_loader_augmented = DataLoader(valid_dataset_augmented, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model \n",
    "model_augmented = diego_net_leo().cuda()\n",
    "\n",
    "optimizer_augmented = torch.optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)\n",
    "\n",
    "# define loss function\n",
    "criterion_augmented = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befc697fab524c6b9bbd01fe7d55e933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model with augmented data\n",
    "use_gpu = True\n",
    "nepochs = 2\n",
    "best_valid = np.inf\n",
    "# writer = SummaryWriter(log_dir=\"/tmp/tensorboard/red_convolucional_simple/\"+str(time()),\n",
    "#                        flush_secs=20)\n",
    "\n",
    "# Enviar modelo a la GPU\n",
    "if use_gpu:\n",
    "    model_augmented = model_augmented.cuda()\n",
    "\n",
    "for k in tqdm_notebook(range(nepochs)):\n",
    "    \n",
    "    # Entrenamiento\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    \n",
    "    for mbdata, mblabel in train_loader_augmented:\n",
    "        \n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model_augmented.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion_augmented(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "        # Calcular gradientes\n",
    "        optimizer_augmented.zero_grad()        \n",
    "        loss.backward()\n",
    "        # Actualizar parámetros\n",
    "        optimizer_augmented.step()\n",
    "    # Enviar información a tensorboard\n",
    "#     writer.add_scalar('Train/Loss', epoch_loss/len(train_idx), k)\n",
    "#     writer.add_scalar('Train/Acc', epoch_acc/len(train_idx), k)\n",
    "    # Validación\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for mbdata, mblabel in valid_loader_augmented:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model_augmented.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion_augmented(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "#     # Enviar información a tensorboard\n",
    "#     writer.add_scalar('Valid/Loss', epoch_loss/len(valid_idx), k)\n",
    "#     writer.add_scalar('Valid/Acc', epoch_acc/len(valid_idx), k)\n",
    "    \n",
    "    if k % 5 == 0:\n",
    "        if epoch_loss/len(valid_idx) < best_valid:\n",
    "            best_valid = epoch_loss/len(valid_idx)\n",
    "            torch.save({'epoca': k,\n",
    "                        'model_state_dict': model_augmented.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer_augmented.state_dict(),                        \n",
    "                        'loss': epoch_loss/len(valid_idx)}, \n",
    "                       'home/leo/models/best_model_augmented.pt') # MODIFICA LA RUTA \n",
    "\n",
    "# Retornar modelo a la CPU\n",
    "if use_gpu:\n",
    "    model_augmented = model_augmented.cpu()\n",
    "    \n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1000\n",
      "           1       0.00      0.00      0.00      1000\n",
      "           2       0.00      0.00      0.00      1000\n",
      "           3       0.00      0.00      0.00      1000\n",
      "           4       0.00      0.00      0.00      1000\n",
      "           5       0.00      0.00      0.00      1000\n",
      "           6       0.00      0.00      0.00      1000\n",
      "           7       0.00      0.00      0.00      1000\n",
      "           8       0.00      0.00      0.00      1000\n",
      "           9       0.10      1.00      0.18      1000\n",
      "\n",
      "    accuracy                           0.10     10000\n",
      "   macro avg       0.01      0.10      0.02     10000\n",
      "weighted avg       0.01      0.10      0.02     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Testing in test loader\n",
    "# test_targets = test_loader.targets.numpy()\n",
    "test_targets = cifar10_test.targets\n",
    "prediction_test_augmented = []\n",
    "for mbdata, label in test_loader:\n",
    "    logits = model_augmented.forward(mbdata)\n",
    "    prediction_test_augmented.append(torch.argmax(logits, dim=1).detach().numpy())\n",
    "\n",
    "prediction_test_augmented = np.concatenate(prediction_test_augmented)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(test_targets, prediction_test_augmented)\n",
    "display(cm)\n",
    "\n",
    "print(classification_report(test_targets, prediction_test_augmented))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparación entrenamiento con y sin aumentación\n",
    "\n",
    "Agregar analisis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuestos (bonus)\n",
    "\n",
    "- Implemente una arquitectura de tipo [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ)\n",
    "- Pruebe la eficacia de Dropout o Regularización L2 en los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
