{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad: Clasificación de imágenes naturales\n",
    "\n",
    "¿Podemos clasificar imágenes naturales usando redes neuronales convolucionales?\n",
    "\n",
    "En esta tarea usaremos la base de datos [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "Responda las preguntas y realice las actividades en cada uno de los bloques\n",
    "\n",
    "Entregas al correo phuijse@inf.uach.cl hasta el Viernes 27, 11:20 AM\n",
    "\n",
    "Se trabajará en grupos de dos personas: se entrega un notebook completo por grupo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base de datos\n",
    "\n",
    "Descargue la base de datos usando `torchvision`\n",
    "\n",
    "- ¿Cuántas categorías existen? \n",
    "\n",
    "Existen 10 categorías dentro del dataset\n",
    "\n",
    "- ¿Cuántos ejemplos hay por categoría?\n",
    "\n",
    "Se tienen 6000 datos por categoría\n",
    "\n",
    "- ¿Cúantos ejemplos hay en el conjunto de entrenamiento? ¿Cúantos hay en el de test?\n",
    "\n",
    "Para los datos de entrenamiento tnemos 50000 datos, y el conjunto de test tendremos 10000\n",
    "\n",
    "- ¿De que tamaño son las imágenes?\n",
    "\n",
    "El tamaño por cada imagen es de 32x32\n",
    "\n",
    "- Muestre 5 ejemplos seleccionados aleatoriamente de cada categoría\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: home/netnavi/datasets\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: home/netnavi/datasets\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='home/netnavi/datasets', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='home/netnavi/datasets', train=False,download=True, transform=torchvision.transforms.ToTensor())\n",
    "display(cifar10_train)\n",
    "display(cifar10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
       "         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
       "         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
       "         ...,\n",
       "         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
       "         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
       "         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n",
       "\n",
       "        [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n",
       "         [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n",
       "         [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n",
       "         ...,\n",
       "         [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n",
       "         [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n",
       "         [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n",
       "\n",
       "        [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n",
       "         [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n",
       "         [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n",
       "         ...,\n",
       "         [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n",
       "         [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n",
       "         [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1961, 0.2000, 0.1647,  ..., 0.1882, 0.0706, 0.0549],\n",
       "         [0.3373, 0.3608, 0.3216,  ..., 0.1804, 0.0667, 0.0431],\n",
       "         [0.1686, 0.1686, 0.2000,  ..., 0.1647, 0.0392, 0.0235],\n",
       "         ...,\n",
       "         [0.8627, 0.8196, 0.7804,  ..., 0.6941, 0.6902, 0.6863],\n",
       "         [0.7373, 0.7137, 0.7137,  ..., 0.6902, 0.6863, 0.6824],\n",
       "         [0.7373, 0.7216, 0.7294,  ..., 0.6902, 0.6902, 0.6784]],\n",
       "\n",
       "        [[0.2510, 0.2471, 0.2157,  ..., 0.1765, 0.0706, 0.0588],\n",
       "         [0.4196, 0.4314, 0.3882,  ..., 0.1686, 0.0667, 0.0471],\n",
       "         [0.2353, 0.2196, 0.2549,  ..., 0.1529, 0.0392, 0.0235],\n",
       "         ...,\n",
       "         [0.6471, 0.6824, 0.6745,  ..., 0.6706, 0.6667, 0.6588],\n",
       "         [0.6980, 0.6667, 0.6314,  ..., 0.6588, 0.6549, 0.6510],\n",
       "         [0.6549, 0.6392, 0.6549,  ..., 0.6627, 0.6588, 0.6471]],\n",
       "\n",
       "        [[0.1451, 0.1608, 0.1608,  ..., 0.1647, 0.0549, 0.0471],\n",
       "         [0.2627, 0.2980, 0.2627,  ..., 0.1608, 0.0549, 0.0392],\n",
       "         [0.1647, 0.1608, 0.1804,  ..., 0.1451, 0.0353, 0.0196],\n",
       "         ...,\n",
       "         [0.5922, 0.6471, 0.6431,  ..., 0.6588, 0.6549, 0.6510],\n",
       "         [0.6353, 0.6353, 0.6157,  ..., 0.6510, 0.6471, 0.6431],\n",
       "         [0.6353, 0.6157, 0.6275,  ..., 0.6510, 0.6510, 0.6353]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0627, 0.0667, 0.0667,  ..., 0.2431, 0.2157, 0.2000],\n",
       "         [0.0627, 0.0627, 0.0588,  ..., 0.2431, 0.2275, 0.2039],\n",
       "         [0.0627, 0.0588, 0.0588,  ..., 0.2235, 0.2314, 0.2196],\n",
       "         ...,\n",
       "         [0.3765, 0.4471, 0.4667,  ..., 0.5020, 0.4706, 0.4588],\n",
       "         [0.4627, 0.3922, 0.4471,  ..., 0.5451, 0.5137, 0.4745],\n",
       "         [0.5647, 0.5333, 0.4118,  ..., 0.5686, 0.5373, 0.5137]],\n",
       "\n",
       "        [[0.2980, 0.3020, 0.3020,  ..., 0.4157, 0.3882, 0.3686],\n",
       "         [0.2980, 0.2980, 0.2941,  ..., 0.4275, 0.4118, 0.3922],\n",
       "         [0.2980, 0.2941, 0.2941,  ..., 0.4314, 0.4353, 0.4275],\n",
       "         ...,\n",
       "         [0.4314, 0.4980, 0.5176,  ..., 0.5294, 0.5098, 0.5137],\n",
       "         [0.5176, 0.4431, 0.4941,  ..., 0.5725, 0.5490, 0.5255],\n",
       "         [0.5804, 0.5490, 0.4471,  ..., 0.5922, 0.5647, 0.5529]],\n",
       "\n",
       "        [[0.2902, 0.2941, 0.2941,  ..., 0.3412, 0.3137, 0.2941],\n",
       "         [0.2902, 0.2902, 0.2902,  ..., 0.3294, 0.3137, 0.2941],\n",
       "         [0.2902, 0.2863, 0.2863,  ..., 0.3098, 0.3137, 0.3059],\n",
       "         ...,\n",
       "         [0.5412, 0.6235, 0.6549,  ..., 0.6000, 0.5686, 0.5569],\n",
       "         [0.6235, 0.5686, 0.6392,  ..., 0.6235, 0.6000, 0.5765],\n",
       "         [0.6784, 0.6588, 0.5608,  ..., 0.6667, 0.6392, 0.6196]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2078, 0.2118, 0.2196,  ..., 0.1843, 0.1608, 0.0941],\n",
       "         [0.1804, 0.2078, 0.2118,  ..., 0.1647, 0.1529, 0.1098],\n",
       "         [0.1765, 0.1961, 0.1804,  ..., 0.1490, 0.1412, 0.1137],\n",
       "         ...,\n",
       "         [0.2784, 0.2902, 0.3137,  ..., 0.2000, 0.1804, 0.1922],\n",
       "         [0.2941, 0.3098, 0.3176,  ..., 0.2392, 0.2510, 0.1882],\n",
       "         [0.3333, 0.3333, 0.3373,  ..., 0.2392, 0.2510, 0.1922]],\n",
       "\n",
       "        [[0.2549, 0.2471, 0.2353,  ..., 0.2000, 0.1765, 0.1098],\n",
       "         [0.2314, 0.2431, 0.2314,  ..., 0.1804, 0.1686, 0.1255],\n",
       "         [0.2314, 0.2353, 0.2039,  ..., 0.1647, 0.1569, 0.1294],\n",
       "         ...,\n",
       "         [0.3255, 0.3255, 0.3333,  ..., 0.2118, 0.1922, 0.1961],\n",
       "         [0.3216, 0.3333, 0.3333,  ..., 0.2549, 0.2627, 0.1961],\n",
       "         [0.3255, 0.3294, 0.3373,  ..., 0.2549, 0.2627, 0.1961]],\n",
       "\n",
       "        [[0.2078, 0.2039, 0.1961,  ..., 0.1961, 0.1725, 0.1059],\n",
       "         [0.1608, 0.1765, 0.1725,  ..., 0.1765, 0.1647, 0.1216],\n",
       "         [0.1490, 0.1608, 0.1333,  ..., 0.1608, 0.1529, 0.1255],\n",
       "         ...,\n",
       "         [0.2588, 0.2588, 0.2627,  ..., 0.1294, 0.1333, 0.1608],\n",
       "         [0.2627, 0.2706, 0.2627,  ..., 0.1608, 0.1882, 0.1608],\n",
       "         [0.2784, 0.2784, 0.2745,  ..., 0.1529, 0.1804, 0.1608]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.5255, 0.4431, 0.3373,  ..., 0.9647, 0.9882, 1.0000],\n",
       "         [0.4118, 0.3137, 0.2706,  ..., 0.9843, 0.9882, 1.0000],\n",
       "         [0.3333, 0.2510, 0.2275,  ..., 0.9647, 0.9490, 0.9647]],\n",
       "\n",
       "        [[1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.2784, 0.2196, 0.1882,  ..., 0.9412, 0.9686, 0.9961],\n",
       "         [0.2000, 0.1294, 0.1490,  ..., 1.0000, 0.9961, 1.0000],\n",
       "         [0.1608, 0.1137, 0.1137,  ..., 0.9412, 0.8980, 0.8784]],\n",
       "\n",
       "        [[1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.1490, 0.1255, 0.0863,  ..., 0.9137, 0.9529, 0.9961],\n",
       "         [0.0863, 0.0549, 0.0549,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0588, 0.0510, 0.0314,  ..., 0.8784, 0.8275, 0.8353]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(cifar10_train[0][0])\n",
    "display(cifar10_train[32][0])\n",
    "display(cifar10_train[50][0])\n",
    "display(cifar10_train[10][0])\n",
    "display(cifar10_train[131][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detalles del dataset\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "\n",
    "Construya `dataloaders` para el conjunto de entrenamiento y validación a partir del dataset `cifar10_train`\n",
    "\n",
    "Construya un `dataloader` para el conjunto de test a partir del dataset `cifar10_test`\n",
    "\n",
    "Use `shuffling` y tamaño de batch 32 para el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set de entrenamiento y validación estratíficados\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(train_size=0.75).split(cifar10_train.data, cifar10_train.targets)\n",
    "train_idx, valid_idx = next(sss)\n",
    "\n",
    "train_dataset = Subset(cifar10_train, train_idx)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "valid_dataset = Subset(cifar10_train, valid_idx)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=256)\n",
    "\n",
    "test_loader = DataLoader(cifar10_test, shuffle=False, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal convolucional\n",
    "\n",
    "Construya una clase que herede de `torch.nn.Module` y que implemente una red neuronal convolucional\n",
    "\n",
    "Tome como base la arquitectura [Lenet5](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) \n",
    "- Tres capas convolucionales cada una seguida de una capa de max-pooling\n",
    "- Dos capas completamente conectadas\n",
    "- Función de activación ReLU en todas las capas excepto en la última \n",
    "- Función de costo `CrossEntropyLoss`\n",
    "- Optimizador `Adam`\n",
    "\n",
    "Entrene su red en la base de datos CIFAR10\n",
    "- Ajuste los parámetros con el conjunto de entrenamiento\n",
    "- Evite el sobreajuste y calibre los hyper-parámetros en el conjunto de validación\n",
    "- Muestre la calidad de su modelo evaluando en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0+cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0798, -0.1499,  0.0486, -0.0823, -0.2386,  0.0506,  0.1516, -0.0090,\n",
       "         -0.0351,  0.2258]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "diego_net_leo(\n",
       "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (mpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=16, out_features=20, bias=True)\n",
       "  (fc2): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "class diego_net_leo(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=20, n_filters=8):\n",
    "        super(diego_net_leo, self).__init__()\n",
    "        # Extracción de características\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=n_filters, kernel_size=3)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=n_filters, out_channels=16, kernel_size=3)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3)\n",
    "        self.mpool = torch.nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "                \n",
    "        # Clasificación\n",
    "        self.fc1 = torch.nn.Linear(in_features=16, out_features=n_hidden)\n",
    "        self.fc2 = torch.nn.Linear(in_features=n_hidden, out_features=10)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"input: \",x.shape)\n",
    "        #return x\n",
    "        z = self.activation(self.conv1(x))\n",
    "        \n",
    "        #print(\"conv 1: \", z.shape)\n",
    "        \n",
    "        z = self.mpool(z)\n",
    "        \n",
    "        #print(\"pool 1: \", z.shape)\n",
    "        \n",
    "        z = self.activation(self.conv2(z))\n",
    "        \n",
    "        #print(\"conv 2: \", z.shape)\n",
    "        \n",
    "        z = self.mpool(z)\n",
    "        \n",
    "        #print(\"pool 2: \", z.shape)\n",
    "        \n",
    "        z = self.activation(self.conv3(z))\n",
    "        \n",
    "        #print(\"conv 3: \", z.shape)\n",
    "        \n",
    "        z = self.mpool(z)\n",
    "        \n",
    "        #print(\"pool 3: \", z.shape)\n",
    "        \n",
    "        z = z.reshape(-1, 16*1*1)\n",
    "        \n",
    "        #print(\"reshape: \", z.shape)\n",
    "        \n",
    "        z = self.activation(self.fc1(z))\n",
    "        \n",
    "        #print(\"fc1: \", z.shape)\n",
    "        \n",
    "        z = self.fc2(z)\n",
    "        \n",
    "        #print(\"fc2: \", z.shape)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "model = diego_net_leo()\n",
    "display(model.forward(cifar10_train[0][0].unsqueeze(0)))\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-325041c90aad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mño\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pop' is not defined"
     ]
    }
   ],
   "source": [
    "pop.ño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from tqdm import tqdm_notebook\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#use_gpu = False\n",
    "nepochs = 1\n",
    "best_valid = np.inf\n",
    "#writer = SummaryWriter(log_dir=\"/tmp/tensorboard/red_convolucional_simple/\", flush_secs=20)\n",
    "\n",
    "# Enviar modelo a la GPU\n",
    "#if use_gpu:\n",
    "#    nnet = nnet.cuda()\n",
    "\n",
    "#for k in tqdm_notebook(range(nepochs)): \n",
    "\n",
    "for k in range(nepochs):\n",
    "\n",
    "    # Entrenamiento\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for mbdata, mblabel in train_loader:\n",
    "        #if use_gpu:\n",
    "        #    mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "        # Calcular gradientes\n",
    "        optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "        # Actualizar parámetros\n",
    "        #optimizer.step()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aumentación de datos\n",
    "\n",
    "Use `torchvision.transforms` para implementar una serie de transformaciones que aumenten el dataset de entrenamiento\n",
    "\n",
    "Compare los resultados de la red entrenada en datos aumentados con la red entrenada en el conjunto regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuestos (bonus)\n",
    "\n",
    "- Implemente una arquitectura de tipo [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ)\n",
    "- Pruebe la eficacia de Dropout o Regularización L2 en los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
